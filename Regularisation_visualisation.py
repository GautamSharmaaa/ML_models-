# -*- coding: utf-8 -*-
"""DL_LAB_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pn45ckic7YIVUThEc-NlhXVxQrXWeNm8
"""

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1, l2

iris = load_iris()
X = iris.data
y = iris.target
y_encoded = to_categorical(y)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42
)

model_baseline = Sequential()
model_baseline.add(Dense(64, activation='tanh', input_shape=(X_train.shape[1],)))
model_baseline.add(Dense(32, activation='tanh'))
model_baseline.add(Dense(16, activation='relu'))
model_baseline.add(Dense(y_train.shape[1], activation='softmax'))
model_baseline.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

history_baseline = model_baseline.fit(
    X_train, y_train,
    epochs=200,
    batch_size=512,
    validation_data=(X_test, y_test),
    verbose=1
)

loss, accuracy = model_baseline.evaluate(X_test, y_test, verbose=0)
print("Baseline Model - Loss: {:.4f}, Accuracy: {:.4f}".format(loss, accuracy))

plt.figure(figsize=(10,6))
plt.plot(history_baseline.history['loss'], label='Training Loss')
plt.plot(history_baseline.history['val_loss'], label='Validation Loss')
plt.title('Baseline Model Loss vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

l1_reg = 0.001
model_l1 = Sequential()
model_l1.add(Dense(64, activation='tanh', input_shape=(X_train.shape[1],),
                   kernel_regularizer=l1(l1_reg)))
model_l1.add(Dense(32, activation='tanh', kernel_regularizer=l1(l1_reg)))
model_l1.add(Dense(16, activation='relu', kernel_regularizer=l1(l1_reg)))
model_l1.add(Dense(y_train.shape[1], activation='softmax'))

model_l1.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

history_l1 = model_l1.fit(
    X_train, y_train,
    epochs=200,
    batch_size=512,
    validation_data=(X_test, y_test),
    verbose=0
)

loss_l1, accuracy_l1 = model_l1.evaluate(X_test, y_test, verbose=0)
print("L1 Regularization Model - Loss: {:.4f}, Accuracy: {:.4f}".format(loss_l1, accuracy_l1))

plt.figure(figsize=(10,6))
plt.plot(history_l1.history['loss'], label='Training Loss')
plt.plot(history_l1.history['val_loss'], label='Validation Loss')
plt.title('L1 Regularization Model Loss vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

l2_reg = 0.001

model_l2 = Sequential()
model_l2.add(Dense(64, activation='tanh', input_shape=(X_train.shape[1],),
                   kernel_regularizer=l2(l2_reg)))
model_l2.add(Dense(32, activation='tanh', kernel_regularizer=l2(l2_reg)))
model_l2.add(Dense(16, activation='relu', kernel_regularizer=l2(l2_reg)))
model_l2.add(Dense(y_train.shape[1], activation='softmax'))

model_l2.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

history_l2 = model_l2.fit(
    X_train, y_train,
    epochs=200,
    batch_size=512,
    validation_data=(X_test, y_test),
    verbose=0
)

loss_l2, accuracy_l2 = model_l2.evaluate(X_test, y_test, verbose=0)
print("L2 Regularization Model - Loss: {:.4f}, Accuracy: {:.4f}".format(loss_l2, accuracy_l2))

plt.figure(figsize=(10,6))
plt.plot(history_l2.history['loss'], label='Training Loss')
plt.plot(history_l2.history['val_loss'], label='Validation Loss')
plt.title('L2 Regularization Model Loss vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

model_dropout = Sequential()
model_dropout.add(Dense(64, activation='tanh', input_shape=(X_train.shape[1],)))
model_dropout.add(Dropout(0.2))
model_dropout.add(Dense(32, activation='tanh'))
model_dropout.add(Dropout(0.2))
model_dropout.add(Dense(16, activation='relu'))
model_dropout.add(Dropout(0.2))
model_dropout.add(Dense(y_train.shape[1], activation='softmax'))

model_dropout.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

history_dropout = model_dropout.fit(
    X_train, y_train,
    epochs=200,
    batch_size=512,
    validation_data=(X_test, y_test),
    verbose=0
)

loss_dropout, accuracy_dropout = model_dropout.evaluate(X_test, y_test, verbose=0)
print("Dropout Regularization Model - Loss: {:.4f}, Accuracy: {:.4f}".format(loss_dropout, accuracy_dropout))

plt.figure(figsize=(10,6))
plt.plot(history_dropout.history['loss'], label='Training Loss')
plt.plot(history_dropout.history['val_loss'], label='Validation Loss')
plt.title('Dropout Regularization Model Loss vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""SCENARIO"""

import numpy as np
import math
import matplotlib.pyplot as plt

# Define the quadratic loss function: f(m) = m^2 - 2*m + 1  = (m - 1)^2
def loss(m):
    return m**2 - 2*m + 1

# Its derivative: f'(m) = 2*m - 2
def grad(m):
    return 2*m - 2

# Cell 2: Implement Momentum optimizer from scratch

# Initialize the weight randomly
w_momentum = np.random.randn()
print("Initial weight for Momentum:", w_momentum)

# Hyperparameters
learning_rate = 0.1
beta = 0.9        # momentum coefficient
tolerance = 1e-8  # convergence tolerance
max_iters = 10000

# Initialize momentum accumulator and iteration counter
v = 0.0
t = 1

# To record the loss and iterations for plotting later
loss_history_momentum = []
iteration_history_momentum = []
w_history_momentum = []

print("\n--- Momentum Optimizer ---")
while t < max_iters:
    # Compute gradient at current weight
    grad_w = grad(w_momentum)

    # Update the momentum term (accumulated gradient)
    v = beta * v + (1 - beta) * grad_w

    # Bias correction: correct the moment estimate using (1 - beta^t)
    v_corrected = v / (1 - beta**t)

    # Update weight using the corrected momentum term
    w_new = w_momentum - learning_rate * v_corrected

    # Compute current loss at the new weight
    current_loss = loss(w_new)

    # Print the updated weight and loss
    print(f"Iteration {t}: weight = {w_new:.10f}, loss = {current_loss:.10f}")

    # Save history for later plotting
    loss_history_momentum.append(current_loss)
    iteration_history_momentum.append(t)
    w_history_momentum.append(w_new)

    # Check convergence: if the change in weight is smaller than the tolerance, break
    if abs(w_new - w_momentum) < tolerance:
        break

    # Update the weight and iteration count
    w_momentum = w_new
    t += 1

print("\nMomentum converged after", t, "iterations.\n")

# Implement RMSProp optimizer from scratch

w_rms = np.random.randn()
print("Initial weight for RMSProp:", w_rms)

learning_rate = 0.1
beta_rms = 0.9
epsilon = 1e-8
tolerance = 1e-8
max_iters = 10000

s = 0.0
t = 1

loss_history_rms = []
iteration_history_rms = []
w_history_rms = []

print("\n--- RMSProp Optimizer ---")
while t < max_iters:
    # Compute the gradient at current weight
    grad_w = grad(w_rms)

    # Update the squared gradient accumulator
    s = beta_rms * s + (1 - beta_rms) * (grad_w**2)

    # Bias correction for the accumulator
    s_corrected = s / (1 - beta_rms**t)

    # Update the weight using the RMSProp formula
    w_new = w_rms - learning_rate * grad_w / (math.sqrt(s_corrected) + epsilon)

    # Compute current loss
    current_loss = loss(w_new)

    # Print the updated weight and loss
    print(f"Iteration {t}: weight = {w_new:.10f}, loss = {current_loss:.10f}")

    # Save history
    loss_history_rms.append(current_loss)
    iteration_history_rms.append(t)
    w_history_rms.append(w_new)

    # Convergence check
    if abs(w_new - w_rms) < tolerance:
        break

    # Update weight and iteration counter
    w_rms = w_new
    t += 1

print("\nRMSProp converged after", t, "iterations.\n")

# Cell 4: Implement Adam optimizer from scratch

# Reinitialize weight for Adam
w_adam = np.random.randn()
print("Initial weight for Adam:", w_adam)

# Hyperparameters for Adam
learning_rate = 0.1
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8
tolerance = 1e-8
max_iters = 10000


m_t = 0.0
v_t = 0.0
t = 1

loss_history_adam = []
iteration_history_adam = []
w_history_adam = []

print("\n--- Adam Optimizer ---")
while t < max_iters:
    # Compute gradient at current weight
    grad_w = grad(w_adam)

    # Update biased first moment estimate
    m_t = beta1 * m_t + (1 - beta1) * grad_w
    # Update biased second moment estimate
    v_t = beta2 * v_t + (1 - beta2) * (grad_w**2)

    # Compute bias-corrected first and second moments
    m_hat = m_t / (1 - beta1**t)
    v_hat = v_t / (1 - beta2**t)

    # Update weight using Adam update rule
    w_new = w_adam - learning_rate * m_hat / (math.sqrt(v_hat) + epsilon)

    # Compute current loss
    current_loss = loss(w_new)

    # Print the updated weight and loss
    print(f"Iteration {t}: weight = {w_new:.10f}, loss = {current_loss:.10f}")

    # Save history
    loss_history_adam.append(current_loss)
    iteration_history_adam.append(t)
    w_history_adam.append(w_new)

    # Check convergence
    if abs(w_new - w_adam) < tolerance:
        break

    # Update weight and iteration count
    w_adam = w_new
    t += 1

print("\nAdam converged after", t, "iterations.\n")

# Cell 5: Plot the loss versus iterations for each optimizer

plt.figure(figsize=(10, 6))
plt.plot(iteration_history_momentum, loss_history_momentum, label='Momentum')
plt.plot(iteration_history_rms, loss_history_rms, label='RMSProp')
plt.plot(iteration_history_adam, loss_history_adam, label='Adam')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Loss vs. Iteration for Momentum, RMSProp, and Adam Optimizers')
plt.legend()
plt.grid(True)
plt.show()

# Cell 6: Compare the number of iterations and final loss for each optimizer

iters_momentum = iteration_history_momentum[-1]
iters_rms = iteration_history_rms[-1]
iters_adam = iteration_history_adam[-1]

final_loss_momentum = loss_history_momentum[-1]
final_loss_rms = loss_history_rms[-1]
final_loss_adam = loss_history_adam[-1]

print("Momentum converged in", iters_momentum, "iterations with final loss:", final_loss_momentum)
print("RMSProp converged in", iters_rms, "iterations with final loss:", final_loss_rms)
print("Adam converged in", iters_adam, "iterations with final loss:", final_loss_adam)

# Determine the best optimizer based on (1) fewer iterations and (2) lower final loss.
# (Since our function has a unique minimum at 0, all optimizers should ideally approach 0 loss.)
if iters_momentum <= iters_rms and iters_momentum <= iters_adam:
    best_optimizer = "Momentum"
    best_iters = iters_momentum
    best_loss = final_loss_momentum
elif iters_rms <= iters_momentum and iters_rms <= iters_adam:
    best_optimizer = "RMSProp"
    best_iters = iters_rms
    best_loss = final_loss_rms
else:
    best_optimizer = "Adam"
    best_iters = iters_adam
    best_loss = final_loss_adam

print("\nBest optimizer: {} (Iterations: {}, Final Loss: {:.10f})".format(best_optimizer, best_iters, best_loss))

"""# Part 2"""

import tensorflow as tf
import time
import numpy as np
import tensorflow.keras.backend as K
import matplotlib.pyplot as plt
from tensorflow.keras.applications import VGG16, VGG19, ResNet50
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Flatten, Dropout, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator

def load_dataset(dataset_name):
    if dataset_name == "cifar10":
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
        num_classes = 10
    elif dataset_name == "cifar100":
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()
        num_classes = 100
    elif dataset_name == "mnist":
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
        x_train = np.stack((x_train,)*3, axis=-1)
        x_test = np.stack((x_test,)*3, axis=-1)
        x_train = tf.image.resize(x_train, (32, 32)).numpy()
        x_test = tf.image.resize(x_test, (32, 32)).numpy()
        num_classes = 10
    elif dataset_name == "fashion_mnist":
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
        x_train = np.stack((x_train,)*3, axis=-1)
        x_test = np.stack((x_test,)*3, axis=-1)
        x_train = tf.image.resize(x_train, (32, 32)).numpy()
        x_test = tf.image.resize(x_test, (32, 32)).numpy()
        num_classes = 10
    else:
        raise ValueError("Unsupported dataset")

    x_train, x_test = x_train / 255.0, x_test / 255.0
    y_train = tf.keras.utils.to_categorical(y_train, num_classes)
    y_test = tf.keras.utils.to_categorical(y_test, num_classes)
    return (x_train, y_train), (x_test, y_test), num_classes

def create_model(model_name, input_shape, num_classes):
    if model_name == "VGG16":
        base_model = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)
    elif model_name == "VGG19":
        base_model = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)
    elif model_name == "ResNet50":
        base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)
    else:
        raise ValueError("Unsupported model")

    base_model.trainable = False
    model = Sequential([
        base_model,
        Flatten(),
        Dense(256, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def measure_inference_time(model, x_test):
    start_time = time.time()
    model.predict(x_test[:100])
    end_time = time.time()
    return (end_time - start_time) / 100

def train_and_evaluate(dataset_name, model_name):
    (x_train, y_train), (x_test, y_test), num_classes = load_dataset(dataset_name)
    input_shape = x_train.shape[1:]
    model = create_model(model_name, input_shape, num_classes)

    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=32)
    test_loss, test_acc = model.evaluate(x_test, y_test)

    inference_time = measure_inference_time(model, x_test)

    return test_acc, inference_time

datasets = ["fashion_mnist", "mnist",  "cifar10", "cifar100"]
models = ["VGG16", "VGG19", "ResNet50"]

results = []
for dataset in datasets:
    for model in models:
        acc, inference_time = train_and_evaluate(dataset, model)
        results.append((dataset, model, acc, inference_time))

dataset_names, model_names, accs, times = zip(*results)

fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].barh(model_names, accs, color='blue')
ax[0].set_xlabel('Accuracy')
ax[0].set_title('Model Accuracy')
ax[1].barh(model_names, times, color='red')
ax[1].set_xlabel('Inference Time (s/sample)')
ax[1].set_title('Model Inference Time')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = results

datasets = list(set(row[0] for row in data))
models = list(set(row[1] for row in data))

accuracy = np.array([row[2] for row in data])
inference_time = np.array([row[3] for row in data])

plt.figure(figsize=(12, 6))
sns.barplot(x=[row[0] for row in data], y=[row[2] for row in data], hue=[row[1] for row in data])
plt.xlabel("Dataset")
plt.ylabel("Accuracy (%)")
plt.title("Model Accuracy Across Different Datasets")
plt.legend(title="Model")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(12, 6))
sns.lineplot(x=[row[0] for row in data], y=[row[3] for row in data], hue=[row[1] for row in data], marker="o")
plt.xlabel("Dataset")
plt.ylabel("Inference Time (s)")
plt.title("Inference Time Across Different Models")
plt.legend(title="Model")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x=inference_time, y=accuracy, hue=[row[1] for row in data], style=[row[0] for row in data], s=100)
plt.xlabel("Inference Time (s)")
plt.ylabel("Accuracy (%)")
plt.title("Accuracy vs Inference Time Trade-off")
plt.legend(title="Model & Dataset")
plt.grid(True)
plt.show()

